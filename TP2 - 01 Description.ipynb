{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TP2 - 01 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of TP\n",
    "\n",
    "In this TP you will develop the full **supervised learning pipeline** including the *hyper-parameter tuning* and *model evalutaion*.  \n",
    "\n",
    "You will then apply the pipeline to three algorithms\n",
    "* nearest neighbour\n",
    "* decision tree\n",
    "* default classifier\n",
    "\n",
    "Finally, you will perform *model comparison* and **discuss** its results.\n",
    "\n",
    "### Recommendation:\n",
    "The code you will develop in this TP is to be re-used in TP3 and the exam.  \n",
    "Therefore we recommend you try to make it clear (use comments, when printing say what you print) so that next time it is easier for you to remember what it does.  \n",
    "Also, try to make the code generic so that it can be easilly used for different datasets.   \n",
    "Try to automate as much as possible so that the code does not require too much of your attention, finally you will need to do the same type of analysis not for 3 algorithms but for 5-6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "You will be workig with the same cars dataset as in TP1.  \n",
    "Each group shall be using the same `brands` as in TP1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Remember that after loading the dataset, there are several preprocessing steps you need to do before trainign the algorithm.\n",
    "If you are not sure what these are, see *Course 8 - 02 Hyper-parameter tuning*.\n",
    "\n",
    "When writing the code, **put short comments explaining what the pre-processing steps are and why you need to do them**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for model evaluation and hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits for model evaluation (training and testing)\n",
    "\n",
    "You will need to write the code splitting the data to training set (used for model learning and hyper-parameter tuning) and test set used for final model evaluation (test error).\n",
    "\n",
    "Here, you can choose to **use either 5-folds cross-validation or 5 time repeated hold-out method.**\n",
    "\n",
    "**Tell us what your choice is and why**. Both choices are good, we just want to know that you understand the differences and you have thought about them.\n",
    "\n",
    "Remember that in the end this procedure will be used for all your algorithms and that these should work over the same train/test splits. You can make sure this will be the case by fixing the seed for the random sample generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits for hyper-parameter tuning\n",
    "\n",
    "Here we want you to **use 3-folds inner cross validation**.\n",
    "\n",
    "You will need to write the code to split each of the training sets above to train/validation accoridng to the 3-fold cross-validation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization accuracy\n",
    "\n",
    "You will also need to prepere the code that will use the trained models to produce predictions for the test instances, calculate the accuracy of over each test set, and calculate the final average accuracy over all the test instances (estimate of generalization accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test nearest neigbour model\n",
    "\n",
    "Once you have the general procedure in place, train the nearest neigbour model.\n",
    "\n",
    "### Hyper-parameter search\n",
    "\n",
    "Hyper-parameter in nearest neighbour algorithm is the number of neighbours to use.\n",
    "We want you to try at least 5 different values. **Tell us which values you decide to try.** (There is no Why questoin here.)\n",
    "\n",
    "Remember that for choosing the best hyper-parameter value, you use the the inner cross validation and the best hyper-parameter is the one with the highest average accuracy over the validation sets.\n",
    "\n",
    "### Model lerning and test accuracy\n",
    "\n",
    "Once you have the best value of the hyper-parameter, you use it to **retrain** the model over the merged train+validation (you do this 5 times, see above *Data splits for model evaluation*). You then use this **retrained** model to get the final test accuracy.\n",
    "\n",
    "For each of the test samples (there should be 5, see above), report the test accuracy and the corresponding hyper-parameter setting (the one chosen as best for this specific split).\n",
    "\n",
    "Are the hyper-parameter parameters the same for all the test sets? **Discuss** if you think this is  normal or not, why it happens and if it creates some difficulties for interpreting the model. **There is no single correct answer here!** We want to see that you undertand the procedure and that you use your brain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test decision tree\n",
    "\n",
    "Use the same general procedure to train a decision tree.\n",
    "\n",
    "Hyper-parameters for decision trees are the pre-prunning criteria such as maximum number of leafs (see *Course 5 - 02 Decision tree prunning*). \n",
    "Pick one of these and use at least 5 different values. **Tell us which one you pick and what values you are using.**\n",
    "\n",
    "Calculate and report the test accuracies together with their corresponding heper-parameter values. (No more comments needed here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test default classifier\n",
    "\n",
    "Default classifier has no hyper-parameters, so you can skip the inner-cross validatoin procedure.\n",
    "\n",
    "Calculate and report the test accuracies for the 5 test sets from the part *Data splits for model evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models\n",
    "\n",
    "Once you have all your test accuracies for the nearest neighbour, decision trees and default classifier, calculate the estimated generalization accuracy of each (the everage accuracy accross the test sets).\n",
    "\n",
    "Is any of the algorithms peforming better than the other two? **Discuss, comment.**\n",
    "\n",
    "## Use the McNemar test \n",
    "\n",
    "Use the McNemar test to verify whether the differences in the generalization accuracy are significant. \n",
    "\n",
    "In McNemar you can always compare only two algorithms. Do all the pair-wise comparisons, present and **explain** the results. Are these what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
